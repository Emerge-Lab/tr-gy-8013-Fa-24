{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework goals: Training neural networks\n",
    "- Get comfortable practicing backprop\n",
    "- Investigate the effect of different optimizers\n",
    "- Implement different initializers and see their effects on deep networks\n",
    "- Learn how to implement the components using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: warmup on a scalar network\n",
    "We are going to revisit the scalar network we implemented previously and implement backprop for it. Then, we're going to generalize it to a much deeper network so that your implementation can't rely on hardcoding without making yourself miserable.\n",
    "$$f[x;\\phi] = \\beta_3 + \\omega_3 \\cdot \\cos \\left[\\beta_2 + \\omega_2 \\cdot \\exp \\left[\\beta_1 + \\omega_1 \\cdot \\sin \\left[\\beta_0 + \\omega_0 \\cdot x \\right] \\right] \\right]$$\n",
    "First, what does this function look like for some random values of the parameters? Let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_0 = 1.0\n",
    "beta_0 = 1.0\n",
    "omega_1 = 1.0\n",
    "beta_1 = 1.0\n",
    "omega_2 = 1.0\n",
    "beta_2 = 1.0\n",
    "omega_3 = 1.0\n",
    "beta_3 = 1.0\n",
    "# $$f[x;\\phi] = \\beta_3 + \\omega_3 \\cdot \\cos \\left[\\beta_2 + \\omega_2 \\cdot \\exp \\left[\\beta_1 + \\omega_1 \\cdot \\sin \\left[\\beta_0 + \\omega_0 \\cdot x] \\right] \\right] \\right]$$\n",
    "def f(x, omega_0, beta_0, omega_1, beta_1, omega_2, beta_2, omega_3, beta_3):\n",
    "    return beta_3 + omega_3 * np.cos(beta_2 + omega_2 * np.exp(beta_1 + omega_1 * np.sin(beta_0 + omega_0 * x)))\n",
    "\n",
    "x_range = np.linspace(-3, 3, 1000)\n",
    "plt.figure()\n",
    "plt.plot(x_range, f(x_range, omega_0, beta_0, omega_1, beta_1, omega_2, beta_2, omega_3, beta_3))\n",
    "\n",
    "# Lets generate a dataset for this and try to fit it with this function\n",
    "# Lets just make it some random polynomial\n",
    "data_size = 256\n",
    "x_data = np.random.uniform(-3, 3, data_size)\n",
    "y_data = f(x_data, omega_0, beta_0, omega_1, beta_1, omega_2, beta_2, omega_3, beta_3) + np.random.normal(0, 0.1, data_size)\n",
    "# plot it real quick\n",
    "plt.figure()\n",
    "plt.scatter(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalarFunction():\n",
    "    def __init__(self, omega_0, beta_0, omega_1, beta_1, omega_2, beta_2, omega_3, beta_3, learning_rate):\n",
    "        self.omega_0 = omega_0\n",
    "        self.beta_0 = beta_0\n",
    "        self.omega_1 = omega_1\n",
    "        self.beta_1 = beta_1\n",
    "        self.omega_2 = omega_2\n",
    "        self.beta_2 = beta_2\n",
    "        self.omega_3 = omega_3\n",
    "        self.beta_3 = beta_3\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Now lets implement backpropagation\n",
    "    def compute_gradients(self, x_data, y_data):\n",
    "        '''Computes and returns an ordered list of the gradients of the loss with respect to each parameter'''\n",
    "        # TODO: this function should return the gradient values of omega_0, beta_0, etc.\n",
    "        # Forward pass\n",
    "        # Backward pass\n",
    "        return dL_domega_0, dL_dbeta_0, dL_domega_1, dL_dbeta_1, dL_domega_2, dL_dbeta_2, dL_domega_3, dL_dbeta_3\n",
    "\n",
    "    def apply_gradients(self, dL_domega_0, dL_dbeta_0, dL_domega_1, dL_dbeta_1, dL_domega_2, dL_dbeta_2, dL_domega_3, dL_dbeta_3, learning_rate):\n",
    "        '''Apply the computed gradients using SGD'''\n",
    "        # TODO fill this in\n",
    "\n",
    "    def update(self, x_data, y_data):\n",
    "        dL_domega_0, dL_dbeta_0, dL_domega_1, dL_dbeta_1, dL_domega_2, dL_dbeta_2, dL_domega_3, dL_dbeta_3 = self.compute_gradients(x_data, y_data)\n",
    "        self.apply_gradients(dL_domega_0, dL_dbeta_0, dL_domega_1, dL_dbeta_1, dL_domega_2, dL_dbeta_2, dL_domega_3, dL_dbeta_3, learning_rate)\n",
    "        \n",
    "    def forward(self, x_data):\n",
    "        '''return the predictions'''\n",
    "        # TODO\n",
    "    \n",
    "    def loss(self, x_data, y_true):\n",
    "        '''return the mean loss'''\n",
    "        # TODO\n",
    "\n",
    "# Now lets train the model\n",
    "omega_0 = 0.5\n",
    "beta_0 = 0.5\n",
    "omega_1 = 0.5\n",
    "beta_1 = 0.5\n",
    "omega_2 = 1.0\n",
    "beta_2 = 1.0\n",
    "omega_3 = 1.0\n",
    "beta_3 = 1.0\n",
    "learning_rate = 0.01\n",
    "\n",
    "func = ScalarFunction(omega_0, beta_0, omega_1, beta_1, omega_2, beta_2, omega_3, beta_3, learning_rate)\n",
    "\n",
    "def plot_and_run(func, num_iters, selection_fn):\n",
    "    # lets quickly plot what this looks like before it all gets started\n",
    "    y_pred = func.forward(x_range)\n",
    "    plt.figure()\n",
    "    plt.plot(x_range, y_pred)\n",
    "    plt.scatter(x_data, y_data)\n",
    "    plt.title('Before training')\n",
    "    plt.show()\n",
    "\n",
    "    # Okay now lets run gradient descent and periodically store the loss for plotting later\n",
    "    losses = []\n",
    "    for i in range(num_iters):\n",
    "        func.update(*selection_fn(x_data, y_data))\n",
    "        losses.append(func.loss(x_data, y_data))\n",
    "        if i % 100 == 0:\n",
    "            print(f'Loss at iteration {i}: {losses[-1]}')\n",
    "\n",
    "    # plot the predictions at the end of training\n",
    "    y_pred = func.forward(x_range)\n",
    "    plt.figure()\n",
    "    plt.plot(x_range, y_pred)\n",
    "    plt.scatter(x_data, y_data)\n",
    "\n",
    "    # plot the loss functions\n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over iterations')\n",
    "\n",
    "def selection_fn(x_data, y_data):\n",
    "    # This function is used to select the data points to use for training\n",
    "    return x_data, y_data\n",
    "\n",
    "plot_and_run(func, 1000, selection_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets see something pretty interesting. Lets try this again but in a setting with different initializations.\n",
    "\n",
    "This will show you some of the challenges that can show up when we're working with very non-convex functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets draw our initializations from a Gaussian distribution\n",
    "omega_0 = np.random.normal(0, 1)\n",
    "beta_0 = np.random.normal(0, 1)\n",
    "omega_1 = np.random.normal(0, 1)\n",
    "beta_1 = np.random.normal(0, 1)\n",
    "omega_2 = np.random.normal(0, 1)\n",
    "beta_2 = np.random.normal(0, 1)\n",
    "omega_3 = np.random.normal(0, 1)\n",
    "beta_3 = np.random.normal(0, 1)\n",
    "learning_rate = 0.01\n",
    "func = ScalarFunction(omega_0, beta_0, omega_1, beta_1, omega_2, beta_2, omega_3, beta_3, learning_rate)\n",
    "plot_and_run(func, 1000, selection_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Nothing to do here, just interesting.\n",
    "So what's happening here? Well, it looks like we're trapped in a local minimum. One way to see this is to plot the loss as a function of the parameters. This is hard to visualize in higher dimension, so let's just plot it in 2D for one set of parameters. We'll take the function we just constructed and sweep the value of $\\omega_0$ and see how it changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_0 = np.random.normal(0, 1)\n",
    "beta_0 = np.random.normal(0, 1)\n",
    "omega_1 = np.random.normal(0, 1)\n",
    "beta_1 = np.random.normal(0, 1)\n",
    "omega_2 = np.random.normal(0, 1)\n",
    "beta_2 = np.random.normal(0, 1)\n",
    "omega_3 = np.random.normal(0, 1)\n",
    "beta_3 = np.random.normal(0, 1)\n",
    "learning_rate = 0.01\n",
    "func = ScalarFunction(omega_0, beta_0, omega_1, beta_1, omega_2, beta_2, omega_3, beta_3, learning_rate)\n",
    "\n",
    "omega_0_range = np.linspace(-5, 5, 100)\n",
    "losses = []\n",
    "original_value = func.omega_0\n",
    "for val in omega_0_range:\n",
    "    func.omega_0 = val\n",
    "    losses.append(func.loss(x_data, y_data))\n",
    "func.omega_0 = original_value\n",
    "plt.figure()\n",
    "plt.plot(omega_0_range, losses)\n",
    "plt.xlabel('omega_0')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of omega_0')\n",
    "\n",
    "# Lets try the same thing for omega 1\n",
    "original_value = func.omega_1\n",
    "losses = []\n",
    "for val in omega_0_range:\n",
    "    func.omega_1 = val\n",
    "    losses.append(func.loss(x_data, y_data))\n",
    "func.omega_1 = original_value\n",
    "plt.figure()\n",
    "plt.plot(omega_0_range, losses)\n",
    "plt.xlabel('omega_0')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of omega_0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we just follow the gradient, we're almost certainly going to get trapped in a local minimum. The point of this is just to recognize that there are going to be non-convex functions that are pretty hard to optimize / have tons of local minima! We're fortunate that it appears that neural networks, despite also likely having tons of local minima, appear to empirically perform well despite falling into them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a selection function that iterates over the data points and implements stochastic gradient descent. It should have a few features\n",
    "# 1. It should have a parameter that controls the number of data points to use\n",
    "# 2. It should sequentially pass over the data points. Once it reaches the end of the data, it should shuffle the data and start over\n",
    "# a few suggestions for how to do this\n",
    "# 1. You can store the current index in the selection function and increment it each time the function is called\n",
    "# 2. You can maintain an array of indices that you shuffle each time you reach the end of the data\n",
    "# 3. One useful thing to know, in python if a class has a __call__ method, you can call the class like a function. We use this to override the () operator so that when the training loop calls the selection function\n",
    "# Note that we've made the dataset divisible by powers of 2 to make it so that\n",
    "# you can keep easy count of how far along the dataset you are\n",
    "\n",
    "\n",
    "class SelectionFunction():\n",
    "    def __init__(self, batch_size, data_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(data_size)\n",
    "        self.current_index = 0\n",
    "        \n",
    "    def __call__(self, x_data, y_data):\n",
    "        # TODO\n",
    "    \n",
    "sgd_selection_func = SelectionFunction(32, x_data.shape[0])\n",
    "learning_rate = 0.01\n",
    "func = ScalarFunction(omega_0, beta_0, omega_1, beta_1, omega_2, beta_2, omega_3, beta_3, learning_rate)\n",
    "plot_and_run(func, 5000, sgd_selection_func)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Implementing a deep network\n",
    "Okay, so in the previous problem we implemented a scalar network but with a pretty bad software abstraction. Unless you figured out a nice abstraction, you probably had to laboriously write out the forward and backward pass for each layer in a fairly large chunk of code.\n",
    "Now, we're going to implement a deeper network where you can't get away with hardcoding the layers. Implement a deep network with 3 layers, each with a hidden size of 32. Use the same nonlinearity in each layer. The correct software abstraction for this isn't obvious, so we'll help you with some components. \n",
    "\n",
    "Note, what follows is a slightly simplified version of the code in Karpathy's [micrograd project](https://github.com/karpathy/micrograd) so just giving credit where it is due. The code in that project implements something much more general and flexible and you should take a look at it if you're interested in understanding a little bit more deeply. It's only 150 lines of code and I recommend reading it!\n",
    "\n",
    "The useful abstraction that we'll give you is the following: a `Module` class with `__call__`, `parameters`, and `zero_grad` methods. The `__call__` method should take in an input and return the output but most importantly, it should store all the info that we are going to need to compute the gradient when we do the backward pass. The `parameters` method should return a list of tensors that are part of the network. The `zero_grad` method should zero out the gradient for each parameter. Every Module will also have a `backward` method that will compute the gradients of the input. Note that this backward method will take in a `grad` argument which will represent the gradient coming from the previous layer in the backwards chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredLoss():\n",
    "    def __init__(self):\n",
    "        self.loss_val = 0\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        # TODO implement this\n",
    "    def grad(self, y_pred, y_true):\n",
    "        # TODO implement this\n",
    "    \n",
    "class ReLU():\n",
    "    def __call__(self, x):\n",
    "        # TODO implement this\n",
    "    def backward(self, grad):\n",
    "        # TODO implement this\n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "    def parameters(self):\n",
    "        return []  # ReLu has no parameters\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, in_features, out_features):\n",
    "        '''Initializer for the layer'''\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = # TODO\n",
    "        self.bias = # TODO\n",
    "        self.weight_grad = # TODO\n",
    "        self.bias_grad = # TODO\n",
    "        \n",
    "    def parameters(self):\n",
    "        '''Return the current parameters and their gradients'''\n",
    "        return [(self.weight, self.weight_grad), (self.bias, self.bias_grad)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        '''Compute the forward pass and store the input for the backward pass'''\n",
    "        # TODO\n",
    "\n",
    "    def backward(self, grad):\n",
    "        '''Compute the gradients and store them in a weight_grad and bias_grad variables. Return the gradients that will be needed for the next layer in the backward pass'''\n",
    "        # TODO\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        '''Set the gradients to zero'''\n",
    "        # TODO\n",
    "        \n",
    "class Network():\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def parameters(self):\n",
    "        '''Returns a list of list of all the parameters in the network'''\n",
    "        # TODO\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        '''Returns the final output of the network'''\n",
    "        # TODO\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        '''Goes backwards through the net and alls the backward function of each layer'''\n",
    "        # TODO\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        '''Goes backward through the net and calls the zero_grad function of each layer'''\n",
    "        # TODO\n",
    "    \n",
    "# Now the last thing we need is an optimizer. This takes in the daa\n",
    "# passes it through the network, computes the loss, and then\n",
    "# actually updates the gradients\n",
    "\n",
    "class SGDOptimizer():\n",
    "    def __init__(self, network, loss_fn, learning_rate):\n",
    "        self.network = network\n",
    "        self.loss_fn = loss_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def step(self, x_data, y_data):\n",
    "        '''Update the parameters of the network and return the loss'''\n",
    "        # TODO implement this\n",
    "        \n",
    "# Now lets try this over the dataset we build before\n",
    "omega_0 = 1.0\n",
    "beta_0 = 1.0\n",
    "omega_1 = 1.0\n",
    "beta_1 = 1.0\n",
    "omega_2 = 1.0\n",
    "beta_2 = 1.0\n",
    "omega_3 = 1.0\n",
    "beta_3 = 1.0\n",
    "data_size = 256\n",
    "x_data = np.random.uniform(-3, 3, (data_size, 1))\n",
    "y_data = f(x_data, omega_0, beta_0, omega_1, beta_1, omega_2, beta_2, omega_3, beta_3) + np.random.normal(0, 0.1, (data_size, 1))\n",
    "\n",
    "# instantiate a network\n",
    "layer1 = Layer(1, 32)\n",
    "layer2 = Layer(32, 32)\n",
    "layer3 = Layer(32, 1)\n",
    "relu = ReLU()\n",
    "network = Network([layer1, relu, layer2, relu, layer3])\n",
    "loss_fn = MeanSquaredLoss()\n",
    "optimizer = SGDOptimizer(network, loss_fn, 0.001)\n",
    "\n",
    "# Now lets train the network\n",
    "losses = []\n",
    "for i in range(3000):\n",
    "    loss = optimizer.step(x_data, y_data)\n",
    "    losses.append(loss)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Loss at iteration {i}: {loss}')\n",
    "        \n",
    "# Now lets output a prediction\n",
    "y_pred = network(x_data)\n",
    "plt.figure()\n",
    "plt.scatter(x_data, y_pred)\n",
    "plt.scatter(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.2 - Changing the activation\n",
    "Now that you have a working implementation, lets try swapping out the ReLU with a leaky ReLU. This is a version of the ReLU that allows a small gradient when the input is negative. This is the leaky ReLU activation function:\n",
    "$$\\text{leaky\\_relu}(x) = \\begin{cases}\n",
    "x, & \\text{if }\\, x > 0 \\\\\n",
    "\\alpha x, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "where $\\alpha$ is a positive hyperparameter that determines how much the function leaks.\n",
    "Lets try swapping out the activation function and see how it affects the convergence of the network (note, I don't expect this to have a big effect).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the leaky relu, train a model with it, and plot the results below\n",
    "class LeakyReLU():\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    def __call__(self, x):\n",
    "        # TODO\n",
    "    def backward(self, grad):\n",
    "        # TODO\n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# Now lets try this with the leaky relu\n",
    "layer1 = Layer(1, 32)\n",
    "layer2 = Layer(32, 32)\n",
    "layer3 = Layer(32, 1)\n",
    "leaky_relu = LeakyReLU(0.01)\n",
    "network = Network([layer1, leaky_relu, layer2, leaky_relu, layer3])\n",
    "loss_fn = MeanSquaredLoss()\n",
    "optimizer = SGDOptimizer(network, loss_fn, 0.001)\n",
    "\n",
    "# Now lets train the network\n",
    "losses = []\n",
    "for i in range(3000):\n",
    "    loss = optimizer.step(x_data, y_data)\n",
    "    losses.append(loss)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Loss at iteration {i}: {loss}')\n",
    "        \n",
    "# Now lets output a prediction\n",
    "y_pred = network(x_data)\n",
    "plt.figure()\n",
    "plt.scatter(x_data, y_pred)\n",
    "plt.scatter(x_data, y_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.3 - Changing the optimizer\n",
    "Now that you have a working implementation, lets try swapping out the optimizer. Try using the Adam optimizer instead of the SGD optimizer. Adam is a more sophisticated optimizer that uses the first and second moments of the gradient to adaptively adjust the learning rate for each parameter. You can find the Adam update rule in the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the Adam optimizer, implement the momentum optimizer\n",
    "# and compare the results below with an SGD optimizer. You should\n",
    "# track the losses for each along the way in variables called losses_adam, losses_sgd, losses_momentum.\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses_adam, label='Adam')\n",
    "plt.plot(losses_sgd, label='SGD')\n",
    "plt.plot(losses_momentum, label='Momentum')\n",
    "# put it on log scale\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over iterations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning: your results may not exactly match mine depending on how you implemented things!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.4 - Changing the initialization\n",
    "Now that you have a working implementation, lets try swapping out the initialization. Try using the Kaiming initialization, discussed in class, instead of the normal distribution. This initialization sets the variance of the weights to be $2/\\text{fan\\_in}$ where $\\text{fan\\_in}$ is the number of incoming connections to a unit. This is a common initialization for ReLU units. You can find the formula for the Kaiming initialization in the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the two networks by implementing a layer that has supprt for different weight initialization schemes, plot them and compare the losses\n",
    "# It is probably easier to just implement one layer but have it take\n",
    "# in a parameter that determines the initialization scheme\n",
    "\n",
    "\n",
    "\n",
    "# Plot the losses on a log plot\n",
    "plt.figure()\n",
    "plt.plot(losses_kaiming, label='Kaiming')\n",
    "plt.plot(losses_normal, label='Normal')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over iterations')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that one won't obviously be better than the other and you **will not necessarily get the same results as me based on small implementation details**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.5 - Reimplementing everything with Torch\n",
    "Now I've made your life difficult by making you implement everything from scratch. I did this for a reason, because this is the fundamental operation that we're doing and it's important to peek under the hood every now and then and see what's going on. But, going forwards, we're mostly going to be focusing on engineering so now it's time to get comfortable with PyTorch. Reimplement the scalar network and the deep network using PyTorch. \n",
    "\n",
    "Some useful things:\n",
    "- [Pytorch tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
    "- [Another pytorch tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html)\n",
    "\n",
    "**Desired output: Plot of the loss going down, plot of the predictions**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_data, dtype=torch.float32)\n",
    "\n",
    "# TODO: implement a network, called deep_network\n",
    "# using pytorch and train it. Run the visualizations\n",
    "# below. Make sure to store the losses for plotting!\n",
    "losses = []\n",
    "            \n",
    "# Now lets output a prediction and plot the losses\n",
    "y_pred = deep_network(x_tensor).detach().numpy()\n",
    "plt.figure()\n",
    "plt.scatter(x_data, y_pred)\n",
    "plt.scatter(x_data, y_data)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Lets try a harder problem, one with some generalization challenges\n",
    "So as we discussed previously, neural networks can fit noise which means sometimes they can struggle to generalize. So lets try some of the regularization methods that we have learned about. We're going to generate a noisy dataset and then split it into a train, validation and test dataset. We're going to try each of the following and see their effect on the test loss:\n",
    "- L2 regularization\n",
    "- Dropout\n",
    "- Ensembling\n",
    "\n",
    "**Before you do anything, we'll need scikit learn so run**\n",
    "```bash\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "Below, we generate some data. There's nothing to do there, just run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make the moon dataset\n",
    "import torch\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "make_moons_data = make_moons(n_samples=1000, noise=0.0)\n",
    "# Now plot it\n",
    "plt.figure()\n",
    "plt.scatter(make_moons_data[0][:, 0], make_moons_data[0][:, 1], c=make_moons_data[1])\n",
    "plt.title('Moon dataset')\n",
    "plt.show()\n",
    "\n",
    "# However, we actually want to do it with a bunch of noise\n",
    "make_moons_data = make_moons(n_samples=256, noise=0.3)\n",
    "# Now plot it\n",
    "plt.figure()\n",
    "plt.scatter(make_moons_data[0][:, 0], make_moons_data[0][:, 1], c=make_moons_data[1])\n",
    "plt.title('Moon dataset')\n",
    "plt.show()\n",
    "# Now split it into training, validation, and testing\n",
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "train_data = make_moons_data[0][:int(train_size * len(make_moons_data[0]))]\n",
    "train_labels = make_moons_data[1][:int(train_size * len(make_moons_data[0]))]\n",
    "val_data = make_moons_data[0][int(train_size * len(make_moons_data[0])):int((train_size + val_size) * len(make_moons_data[0]))]\n",
    "val_labels = make_moons_data[1][int(train_size * len(make_moons_data[0])):int((train_size + val_size) * len(make_moons_data[0]))]\n",
    "test_data = make_moons_data[0][int((train_size + val_size) * len(make_moons_data[0])):]\n",
    "test_labels = make_moons_data[1][int((train_size + val_size) * len(make_moons_data[0])):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.1 - Implement a neural network on this and visualize the train, validation and test loss\n",
    "Keep in mind that for this problem, you'll need a probability at the end, not a scalar value. So, here we're going to be implementing logistic regression! Instead of just taking the sigmoid like we normally would, there's actually a torch loss that can directly take scalar outputs and applies a sigmoid to it to turn it into a probability. This loss is called the torch.nn.BCEWithLogitsLoss. Documentation for it is [here](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html). Train for long enough that you observe your validation loss actually start to go up. An alternate version is to apply the sigmoid (converting it to a probability) and then use the torch.nn.BCELoss.\n",
    "\n",
    "A few more comments:\n",
    "1. Please do not train on the entire dataset. In each optimizer step, take a subset of the data, say 32 points. This will make your training faster. You can implement this manually by shuffling your data, stepping through it in steps of 32, and then reshuffling when you reach the end. Or, you can also use [Torch Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). Up to you! \n",
    "2. Use the ADAM optimizer.\n",
    "3. For your network, add a function called logit that will take in a dataset and return the logit (i.e. the value before the sigmoid is applied to turn it into a probability). Some of the plotting code relies on this.\n",
    "4. The losses will fluctuate a lot so I **highly** recommend smoothing them out over the entire epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# TODO: Set up and train a neural network on this data\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
    "val_data = torch.tensor(val_data, dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.float32)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.float32)\n",
    "\n",
    "def visualize_results(train_losses, val_losses, deep_network):\n",
    "    '''Helpful visualizer utility\n",
    "    Args: train_losses: list of training losses\n",
    "            val_losses: list of validation losses\n",
    "            deep_network: the trained network\n",
    "    '''\n",
    "    # Now lets evaluate the model on the test set\n",
    "    test_loader = torch.utils.data.DataLoader(list(zip(test_data, test_labels)), batch_size=32, shuffle=False)\n",
    "    test_losses = []\n",
    "    deep_network.eval()\n",
    "    test_loss_total = 0\n",
    "    total_test_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            y_pred = deep_network(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch.unsqueeze(1))\n",
    "            test_loss_total += loss.item() * x_batch.size(0)\n",
    "            total_test_samples += x_batch.size(0)\n",
    "    average_test_loss = test_loss_total / total_test_samples\n",
    "    print(f'Average test loss: {average_test_loss}')\n",
    "\n",
    "    # Now lets output a prediction and plot the losses\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    # plot the test loss as a line\n",
    "    plt.axhline(average_test_loss, color='r', label='Test')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    # Now lets plot the predictions and visualize the decision boundary\n",
    "    x_range = np.linspace(-3, 3, 100)\n",
    "    y_range = np.linspace(-3, 3, 100)\n",
    "    x_mesh, y_mesh = np.meshgrid(x_range, y_range)\n",
    "    xy_pairs = np.stack([x_mesh, y_mesh], axis=-1).reshape(-1, 2)\n",
    "    xy_tensor = torch.tensor(xy_pairs, dtype=torch.float32)\n",
    "    deep_network.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = deep_network(xy_tensor).numpy().reshape(100, 100)\n",
    "    plt.figure()\n",
    "    plt.contourf(x_mesh, y_mesh, predictions, levels=0)\n",
    "    plt.scatter(test_data[:, 0], test_data[:, 1], c=test_labels)\n",
    "    plt.title('Decision boundary')\n",
    "\n",
    "\n",
    "# TODO: Implement the network and train it. Make sure to track\n",
    "# the train and value losses along the way and store them in train_losses and val_losses\n",
    "    \n",
    "visualize_results(train_losses, val_losses, deep_network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.2 - Implement L2 regularization\n",
    "Implement L2 regularization. This is done by adding a term to the loss that is proportional to the sum of the squares of the weights. This is a common form of regularization that encourages the weights to be small. The strength of the regularization is controlled by a hyperparameter $\\lambda$. Implement this and see how it affects the test loss. Reuse the previous network from the previous problem and try a couple of different regularization parameters to see their effect. Try it with three different values of $\\lambda$ and see if you can get the validation loss closer to the training loss. \n",
    "\n",
    "However, I will tell you a useful fact. The Adam optimizer has a weight_decay argument that implements L2 regularization. You're welcome to use it. The other way to implement weight decay is to note that the model has a parameters() method that returns a list of all the parameters in the model. You can then loop over these parameters and add the sum of the squares of the weights to the loss.\n",
    "\n",
    "So, as an example for a hypothetical model called deep_model, you could do something like this:\n",
    "```python\n",
    "for param in deep_model.parameters():\n",
    "    do something with the parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement a network with L2 regularization and visualize the predictions\n",
    "# Use this to visualize the results. Make sure to track\n",
    "# the train and value losses along the way.\n",
    "visualize_results(train_losses, val_losses, deep_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.3 - Use dropout\n",
    "Use dropout in your network. This is a form of regularization that randomly sets a fraction of the activations to zero. This can be implemented in PyTorch using the torch.nn.Dropout layer. This layer takes in a single argument, which is the probability that a neuron is set to zero. Note that this is a function, so you apply it to an input or a hidden layer in the forward method so you'll have to modify the network you have.\n",
    "\n",
    "**Important warning**: While you use dropout during training, you may not want to use it at test-time. Dropout is intended as a training-time regularization, but there's not an obvious reason to apply it after you're done training. So, you'll want to turn off dropout during test-time. You can do this by calling `model.eval()` on your model. This will set the model to evaluation mode and turn off dropout. You can then call `model.train()` to turn dropout back on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a network with dropout in the layers and visualize the predictions. Make sure to track the train and value losses along the way.\n",
    "\n",
    "# Use this to visualize the results\n",
    "visualize_results(train_losses, val_losses, deep_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.4 - Ensembling\n",
    "This is really one of the most powerful tools in the toolbox and is pretty much always going to give you an improvement (although it comes at a serious price of having to train and keep around multiple models!) The idea is that you train multiple models on the same dataset and then average their predictions. There's a couple of ways you could do this:\n",
    "1. You could train multiple models on the same dataset and then average their predictions. This is the simplest way to do it.\n",
    "2. You could train multiple models on different slices of the train dataset. \n",
    "\n",
    "**Watch out**: each model will need its own optimizer\n",
    "\n",
    "In the end, you'll want to write a class that takes in a list of models and then averages their predictions (note, you can average their logits or average their actual class predictions. Unclear which is best.). This class should have a `forward` method that takes in an input and returns the mode of the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 5\n",
    "# TODO implement an ensemble network and visualize the predictions. Make sure to track the train and value losses along the way.\n",
    "# Use this to visualize the results\n",
    "visualize_results(train_losses, val_losses, final_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
